{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised model\n",
        "By: Preston Cusick, Daniel Wendland, Ethan Eckmann\n",
        "\n",
        "The goal of this model is to provide an additional assistance for the analysis of our project."
      ],
      "metadata": {
        "id": "gRc0JNM_s2V2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "bM8QV5z3tW5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "7ApESsuBtYt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the churn_predict_B.csv\n",
        "df1 = pd.read_csv('churn_predict_A.csv')\n",
        "df2 = pd.read_csv('churn_predict_B.csv')\n",
        "\n",
        "# Drop rows where 'Probability' is NaN and create a proper copy\n",
        "df_clean1 = df1.dropna(subset=['Probability']).copy()\n",
        "df_clean2 = df2.dropna(subset=['Probability']).copy()\n",
        "\n",
        "# Use only the 'Probability' column\n",
        "X = df_clean1 [['Probability']]\n",
        "\n",
        "# This will standardize data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# KMeans clustering\n",
        "kmeans_model = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "cluster_assignments = kmeans_model.fit_predict(X_scaled)\n",
        "\n",
        "df_clean1.loc[X.index, 'Cluster'] = cluster_assignments\n",
        "df_clean2.loc[X.index, 'Cluster'] = cluster_assignments\n",
        "\n",
        "\n",
        "# Get the average of 'Probability' where 'Cluster' is X\n",
        "# Need to get the average of each cluster and then manually map to risk groups\n",
        "average_a = df_clean1[df_clean1['Cluster'] == 0.0]['Probability'].mean()\n",
        "print(average_a)\n",
        "\n",
        "average_a = df_clean1[df_clean1['Cluster'] == 1.0]['Probability'].mean()\n",
        "print(average_a)\n",
        "\n",
        "average_a = df_clean1[df_clean1['Cluster'] == 2.0]['Probability'].mean()\n",
        "print(average_a)\n",
        "\n",
        "average_a = df_clean1[df_clean1['Cluster'] == 3.0]['Probability'].mean()\n",
        "print(average_a)\n",
        "\n",
        "\n",
        "# Manually map cluster labels\n",
        "label_map = {\n",
        "    0.0: 'Low Risk',\n",
        "    1.0: 'High Risk',\n",
        "    2.0: 'Very Low Risk',\n",
        "    3.0: 'Moderate Risk'\n",
        "}\n",
        "df_clean1['Cluster_Label'] = df_clean1['Cluster'].map(label_map)\n",
        "df_clean2['Cluster_Label'] = df_clean2['Cluster'].map(label_map)\n",
        "\n",
        "# # Drop 'Cluster' column (the 0.0, 1.0, etc.) if needed\n",
        "# df_clean1 = df_clean1.drop(columns=['Cluster'])\n",
        "# df_clean2 = df_clean2.drop(columns=['Cluster'])\n",
        "\n",
        "# Save the clustered data\n",
        "df_clean1.to_csv('churn_clusters_A.csv', index=False)\n",
        "df_clean2.to_csv('churn_clusters_B.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "# Count with high risk\n",
        "count = len(df_clean1[df_clean1['Cluster_Label'] == 'High Risk'])\n",
        "print(count)\n",
        "print(count/(len(df_clean1))*100)\n",
        "print()\n",
        "count = len(df_clean1[df_clean1['Cluster_Label'] == 'Moderate Risk'])\n",
        "print(count)\n",
        "print(count/(len(df_clean1))*100)\n",
        "print()\n",
        "count = len(df_clean1[df_clean1['Cluster_Label'] == 'Low Risk'])\n",
        "print(count)\n",
        "print(count/(len(df_clean1))*100)\n",
        "print()\n",
        "count = len(df_clean1[df_clean1['Cluster_Label'] == 'Very Low Risk'])\n",
        "print(count)\n",
        "print(count/(len(df_clean1))*100)\n",
        "print()\n",
        "\n",
        "average_prob = df_clean1['Probability'].mean()\n",
        "print(f\"Average probability: {average_prob}\")"
      ],
      "metadata": {
        "id": "UGibF2VDQ3Fm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cc7207f-dc2b-4e12-dd9a-b987f36c6142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.45204904614246055\n",
            "0.8317507018562125\n",
            "0.21438711944322747\n",
            "0.6331785787931747\n",
            "595\n",
            "31.835205992509362\n",
            "\n",
            "576\n",
            "30.818619582664525\n",
            "\n",
            "438\n",
            "23.434991974317818\n",
            "\n",
            "260\n",
            "13.911182450508294\n",
            "\n",
            "Average probability: 0.5956878877768603\n"
          ]
        }
      ]
    }
  ]
}